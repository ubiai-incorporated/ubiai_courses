{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4668db48-ebef-4529-ad64-f40435e82153",
      "metadata": {
        "id": "4668db48-ebef-4529-ad64-f40435e82153"
      },
      "source": [
        "# Continuous LLM Improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Introduction\n",
        "\n",
        "Large Language Models are not static artifacts—they exist in a world that never stops changing. Language evolves, new knowledge emerges, and user expectations shift. A model trained just a year ago might already feel outdated, not because it was poorly built, but because the ground beneath it has moved. Think of an LLM like a city's subway map. If the city expands but the map stays frozen in time, what was once a reliable guide becomes a source of frustration.\n",
        "\n",
        "This is the core challenge of continuous LLM improvement. Unlike traditional software, where bugs are discrete and fixes are clear-cut, LLMs degrade in subtle ways. Their knowledge becomes stale, their biases calcify, and their once-sharp reasoning dulls. Left unchecked, even the most advanced models drift into irrelevance or, worse, harm.\n",
        "\n",
        "This chapter explores why continuous improvement isn't optional but existential for LLMs. We'll examine the twin challenges of data and model drift, develop strategies for vigilant monitoring, and build systems that transform users into collaborators in the model's evolution.\n",
        "\n",
        "## 1. The Need for Continuous Improvement in LLMs\n",
        "\n",
        "Traditional systems follow a predictable path: development, testing, deployment. LLMs break this paradigm. Their performance doesn't just depend on code quality, but on their ability to mirror an ever changing world. Five forces make continuous improvement non-negotiable:\n",
        "\n",
        "- **Evolving Language Use**: New slang, terminologies, and cultural shifts alter linguistic patterns.\n",
        "- **Domain Shifts**: Changes in application contexts (e.g., medical, legal, or financial domains) require model adaptation.\n",
        "- **Bias and Fairness**: Societal norms around fairness evolve, necessitating bias mitigation updates.\n",
        "- **Adversarial Attacks**: New attack vectors (e.g., prompt injections) degrade model reliability.\n",
        "- **User Feedback**: Real-world usage reveals edge cases and deficiencies not captured during initial training.\n",
        "\n",
        "The consequence of neglect isn't just stagnation - it's active deterioration. A 2023 Stanford study found that GPT-4's accuracy on medical queries dropped 15% over six months as guidelines evolved.\n",
        "\n",
        "## The Silent Killers of LLM Performance\n",
        "\n",
        "### Data Drift: When the World Outpaces Your Model\n",
        "\n",
        "Data drift is the quiet, insidious force that erodes an LLM's reliability over time. It happens when the language, facts, or context the model encounters no longer match its training data. Imagine a travel guidebook written in 2010—it might still get Paris right, but it won't know about new metro lines or pandemic-era rules. Similarly, an LLM trained pre-2021 might discuss \"blockchain\" fluently but fumble on \"ZK-rollups.\"\n",
        "\n",
        "**Key Signs of Data Drift:**\n",
        "- Gradual decline in user satisfaction (\"answers feel off\")\n",
        "- Sudden breakdowns on new linguistic trends (e.g., slang meaning shifts)\n",
        "- Embeddings of recent queries clustering differently than training data\n",
        "\n",
        "Fixing data drift isn't about constant retraining—that's impractical and expensive. Instead, it's about building feedback loops. Dynamic data pipelines that ingest and annotate fresh text, active learning systems that flag edge cases for human review, and lightweight \"patch\" updates that target specific gaps. The goal is to keep the model's knowledge fluid without rebuilding it from scratch every time the world changes.\n",
        "\n",
        "### Model Drift: When Your LLM 'Forgets'\n",
        "\n",
        "Even if the data stays perfectly static, LLMs can still degrade on their own. This is model drift—a slow unraveling of the model's internal logic, often in ways that aren't obvious until it's too late. Unlike data drift, which happens because the world changes, model drift happens because the model itself changes, and not always for the better.\n",
        "\n",
        "**Common Causes:**\n",
        "- Catastrophic forgetting: Fine-tuning for one task weakens others\n",
        "- Parameter saturation: Models take safer, blander shortcuts over time\n",
        "- Conceptual shifts: Societal changes alter what \"good\" answers look like\n",
        "\n",
        "**Diagnosing it requires more than accuracy metrics. You need to:**\n",
        "- Probe latent spaces for shifting concept representations\n",
        "- Run A/B tests comparing old vs. new model outputs\n",
        "- Watch for \"quieter\" failures (e.g., losing nuance while gaining fluency)\n",
        "\n",
        "The solution isn't to avoid updates—it's to make them smarter. Techniques like Elastic Weight Consolidation (EWC) penalize changes to critical neural pathways, preserving core knowledge while allowing targeted improvements. Modular architectures let you swap out specific components without destabilizing the entire system. The goal is to keep the model adaptable without letting it lose itself in the process.\n",
        "\n",
        "## Monitoring: The Art of Listening to Your LLM\n",
        "\n",
        "Monitoring an LLM isn't like monitoring a server. You're not just watching for crashes or slowdowns—you're watching for subtle shifts in behavior, tone, and reasoning. Traditional software alerts won't catch a model that's become slightly more biased, slightly less creative, or slightly more prone to hallucination. You need a different approach.\n",
        "\n",
        "Start by defining what \"good\" looks like for your specific use case. A customer service bot might prioritize consistency and politeness, while a research assistant needs depth and accuracy. Once you know what matters, instrument your system to track it. Embedding drift detectors can flag when user queries start veering into uncharted territory. Sentiment analyzers can catch if the model's tone becomes unintentionally brusque or overly flowery. And, crucially, you need human-in-the-loop checks—real people reviewing samples of outputs to catch what automated systems miss.\n",
        "\n",
        "But monitoring isn't just about catching problems—it's about understanding them. When the model starts behaving differently, is it because of a data shift, a parameter drift, or something else entirely? The answer determines whether you need a data refresh, a fine-tuning pass, or a deeper architectural rethink.\n",
        "\n",
        "## The Feedback Loop: Turning Users into Teachers\n",
        "\n",
        "The best source of continuous improvement isn't a team of engineers—it's the users themselves. Every time someone corrects an LLM's output, flags an error, or even just sighs and rephrases their query, they're giving you gold. The challenge is harvesting that feedback systematically.\n",
        "\n",
        "**Building Effective Feedback Systems:**\n",
        "- Start simple (thumbs up/down ratings)\n",
        "- Add structured options (error highlighting, answer rewriting)\n",
        "- Balance automation with human curation to avoid gaming\n",
        "\n",
        "The best systems create virtuous cycles: models learn from mistakes, users get better results, and the whole system aligns closer to real needs.\n",
        "\n",
        "Of course, feedback loops can backfire if not designed carefully. Users might game the system (like upvoting incorrect but pleasing answers), or feedback might become skewed toward vocal minorities. The fix is to balance automated collection with human curation, always keeping the end goal in sight: not just a model that pleases users, but one that serves them well.\n",
        "\n",
        "## LLMs as Living Systems\n",
        "\n",
        "Continuous improvement isn't a feature you add to an LLM—it's the only way to keep it alive. Unlike traditional software, which is \"finished\" at launch, LLMs are never done. They exist in dialogue with the world, and the world never stops talking back."
      ],
      "metadata": {
        "id": "RHnwDwN8ewei"
      },
      "id": "RHnwDwN8ewei"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's learn how to do it:"
      ],
      "metadata": {
        "id": "aNA-qFE4e50h"
      },
      "id": "aNA-qFE4e50h"
    },
    {
      "cell_type": "markdown",
      "id": "dfb3e1b0-1ef7-4b48-94a1-3a537b3cbb7f",
      "metadata": {
        "id": "dfb3e1b0-1ef7-4b48-94a1-3a537b3cbb7f"
      },
      "source": [
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8a7305-00ff-4cfc-acb4-cac53eb2505e",
      "metadata": {
        "tags": [],
        "id": "4f8a7305-00ff-4cfc-acb4-cac53eb2505e"
      },
      "outputs": [],
      "source": [
        "%pip install -U mlrun openai transformers datasets trl peft bitsandbytes sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c99e64-b5a2-45c8-83f3-eda2e0d79cb2",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "c1c99e64-b5a2-45c8-83f3-eda2e0d79cb2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "import mlrun\n",
        "from mlrun.features import Feature  # To log the model with inputs and outputs information\n",
        "import mlrun.common.schemas.alert as alert_constants  # To configure an alert\n",
        "from mlrun.model_monitoring.helpers import get_result_instance_fqn  # To configure an alert\n",
        "\n",
        "from src.llm_as_a_judge import OpenAIJudge\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5b5468-baed-4fa9-af38-9c58da507895",
      "metadata": {
        "id": "2b5b5468-baed-4fa9-af38-9c58da507895"
      },
      "source": [
        "### Set Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0e6527-3f48-46d9-b1f3-870ae9a7fe33",
      "metadata": {
        "tags": [],
        "id": "8e0e6527-3f48-46d9-b1f3-870ae9a7fe33"
      },
      "outputs": [],
      "source": [
        "OPENAI_BASE_URL = \"\"\n",
        "OPENAI_API_KEY = \"\"\n",
        "HF_TOKEN = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ecad61-37cb-4edd-8a1f-cda49c8c75e7",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "00ecad61-37cb-4edd-8a1f-cda49c8c75e7"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"OPENAI_BASE_URL\"] = OPENAI_BASE_URL\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a83120de-f797-49f8-be8e-da00b5111f14",
      "metadata": {
        "id": "a83120de-f797-49f8-be8e-da00b5111f14"
      },
      "source": [
        "### Create The Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b964b81f-4ba6-4436-a9fb-6690d157f684",
      "metadata": {
        "id": "b964b81f-4ba6-4436-a9fb-6690d157f684"
      },
      "outputs": [],
      "source": [
        "project = mlrun.get_or_create_project(\n",
        "    name=\"llm-monitoring\",\n",
        "    parameters={\n",
        "        \"default_image\": \"gcr.io/iguazio/llm-serving:1.7.0\",\n",
        "    },\n",
        "    context=\"./src\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "085940f9-f354-49cc-8817-eb91b3193d92",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "085940f9-f354-49cc-8817-eb91b3193d92"
      },
      "outputs": [],
      "source": [
        "\n",
        "project.set_model_monitoring_credentials(\n",
        "    os.environ[\"V3IO_ACCESS_KEY\"],\n",
        "    \"v3io\",\n",
        "    \"v3io\",\n",
        "    \"v3io\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84430c13-34fa-4b6e-9a48-f00b0a6baf27",
      "metadata": {
        "tags": [],
        "id": "84430c13-34fa-4b6e-9a48-f00b0a6baf27"
      },
      "outputs": [],
      "source": [
        "project.enable_model_monitoring(\n",
        "    image=\"mlrun/mlrun\",\n",
        "    base_period=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1221853f-f743-4748-af10-033698ee6c27",
      "metadata": {
        "id": "1221853f-f743-4748-af10-033698ee6c27"
      },
      "source": [
        "## Using LLM as a Judge in LLM Monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5efdd45-1edb-463f-bc7f-1f82b8dd4134",
      "metadata": {
        "id": "d5efdd45-1edb-463f-bc7f-1f82b8dd4134"
      },
      "source": [
        "\n",
        "\n",
        "In the context of LLM monitoring, using a Large Language Model as a **judge** refers to employing another model—often the same or a more advanced version of the deployed model—to evaluate the quality, relevance, and safety of generated outputs. This approach is particularly valuable when human evaluation is costly, slow, or inconsistent.\n",
        "\n",
        "The LLM judge can be prompted to assess model outputs based on predefined criteria such as factual accuracy, coherence, completeness, bias, or adherence to instructions. For example, given a user prompt and the model’s response, the judge LLM can be asked:  \n",
        "> *“Does this answer correctly and thoroughly address the prompt without hallucinating or including unsafe content?”*\n",
        "\n",
        "This method enables scalable, automated, and fine-grained feedback loops that help detect model regressions, fine-tune behavior, and ensure quality control. However, it's essential to be cautious of potential biases the judge model may inherit or reinforce, especially if it's from the same family as the evaluated model.\n",
        "\n",
        "#### Benefits:\n",
        "- Scalable and cost-effective evaluation  \n",
        "- High-speed feedback on performance and safety  \n",
        "- Useful for continuous deployment pipelines  \n",
        "\n",
        "#### Challenges:\n",
        "- Risk of reinforcing existing biases  \n",
        "- Dependence on the quality of prompt engineering  \n",
        "- May require periodic human calibration  \n",
        "\n",
        "Using LLMs as judges is becoming a cornerstone of modern **LLMOps** (LLM Operations), helping teams maintain high-quality standards while iterating quickly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776fa9e3-3b67-44f5-845a-228e106ebe5e",
      "metadata": {
        "id": "776fa9e3-3b67-44f5-845a-228e106ebe5e"
      },
      "source": [
        "### Load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6011439f-c945-4836-87be-5310fea63e59",
      "metadata": {
        "id": "6011439f-c945-4836-87be-5310fea63e59"
      },
      "source": [
        "We'll use a small dataset to teach the model to answer only banking related questions. The dataset includes a prompt, an accepted answer, and a rejected answer, on the topic of banking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5524deb-f4c1-4bf2-8a28-d58f0f382402",
      "metadata": {
        "id": "f5524deb-f4c1-4bf2-8a28-d58f0f382402"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"mlrun/banking-orpo\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "dataset = dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987fd074-b3ce-46ae-9220-94893b51acc7",
      "metadata": {
        "id": "987fd074-b3ce-46ae-9220-94893b51acc7",
        "outputId": "8060c9bd-4875-4d99-e1d5-12c45baaa8d8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>rejected</th>\n",
              "      <th>score</th>\n",
              "      <th>chosen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Which animal is known for its ability to swim against strong ocean currents?</td>\n",
              "      <td>The salmon is known for its ability to swim against strong ocean currents and migrate upstream to their freshwater spawning grounds.</td>\n",
              "      <td>0</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does a credit card work?</td>\n",
              "      <td>A credit card makes money grow in a magic pot each time you swipe it.</td>\n",
              "      <td>1</td>\n",
              "      <td>A credit card is a type of loan where a card issuer extends a line of credit to the cardholder to borrow money for making purchases. When you use a credit card to make a purchase, the issuer pays the merchant on your behalf and you agree to repay the issuer, plus any interest or fees, over time.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In what year did the Mongol warrior Genghis Khan die?</td>\n",
              "      <td>Genghis Khan, the Mongol warrior and founder of the Mongol Empire, is believed to have died in 1227.</td>\n",
              "      <td>0</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the largest species of salamander?</td>\n",
              "      <td>The Chinese giant salamander is considered the largest species of salamander, with adults reaching lengths of up to 5 feet</td>\n",
              "      <td>0</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How to make a budget-friendly 30-minute dinner?</td>\n",
              "      <td>Sauté a pound of ground beef with one chopped onion, green pepper, and minced garlic. Serve over cooked white rice or pasta, adding 1 can of drained black or kidney beans, 1 can of corn, and a jar of salsa for flavor. Top with shredded cheese or sour cream, if desired.</td>\n",
              "      <td>0</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         prompt  \\\n",
              "0  Which animal is known for its ability to swim against strong ocean currents?   \n",
              "1                                                  How does a credit card work?   \n",
              "2                         In what year did the Mongol warrior Genghis Khan die?   \n",
              "3                                    What is the largest species of salamander?   \n",
              "4                               How to make a budget-friendly 30-minute dinner?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                        rejected  \\\n",
              "0                                                                                                                                           The salmon is known for its ability to swim against strong ocean currents and migrate upstream to their freshwater spawning grounds.   \n",
              "1                                                                                                                                                                                                          A credit card makes money grow in a magic pot each time you swipe it.   \n",
              "2                                                                                                                                                                           Genghis Khan, the Mongol warrior and founder of the Mongol Empire, is believed to have died in 1227.   \n",
              "3                                                                                                                                                     The Chinese giant salamander is considered the largest species of salamander, with adults reaching lengths of up to 5 feet   \n",
              "4  Sauté a pound of ground beef with one chopped onion, green pepper, and minced garlic. Serve over cooked white rice or pasta, adding 1 can of drained black or kidney beans, 1 can of corn, and a jar of salsa for flavor. Top with shredded cheese or sour cream, if desired.   \n",
              "\n",
              "   score  \\\n",
              "0      0   \n",
              "1      1   \n",
              "2      0   \n",
              "3      0   \n",
              "4      0   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                     chosen  \n",
              "0                                                                                                                                                                                                     As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  \n",
              "1  A credit card is a type of loan where a card issuer extends a line of credit to the cardholder to borrow money for making purchases. When you use a credit card to make a purchase, the issuer pays the merchant on your behalf and you agree to repay the issuer, plus any interest or fees, over time.  \n",
              "2                                                                                                                                                                                                     As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  \n",
              "3                                                                                                                                                                                                     As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  \n",
              "4                                                                                                                                                                                                     As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = dataset.to_pandas()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de21e5e7-6f19-4353-becf-781437ec2462",
      "metadata": {
        "id": "de21e5e7-6f19-4353-becf-781437ec2462"
      },
      "source": [
        "### Create an Accuracy Metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c681b870-bc99-42d2-9fa5-5b970a4489e5",
      "metadata": {
        "id": "c681b870-bc99-42d2-9fa5-5b970a4489e5"
      },
      "outputs": [],
      "source": [
        "# This function will act as our judge's accuracy\n",
        "def compute_accuracy(col1, col2):\n",
        "    matching_values = sum(col1 == col2)\n",
        "    total_values = len(col1)\n",
        "    return matching_values / total_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c842034c-0693-4a23-a53a-c651801c68f9",
      "metadata": {
        "id": "c842034c-0693-4a23-a53a-c651801c68f9"
      },
      "source": [
        "### Evaluation Set\n",
        "\n",
        "we'll take 10% of the data and split it into two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb8f723-8cc1-43a5-aa27-22e75ea5179b",
      "metadata": {
        "id": "deb8f723-8cc1-43a5-aa27-22e75ea5179b"
      },
      "outputs": [],
      "source": [
        "orpo_dataset = dataset.to_pandas().sample(frac=0.1, random_state=42, ignore_index=True)\n",
        "middle_index = len(orpo_dataset) // 2\n",
        "\n",
        "chosen = (\n",
        "    orpo_dataset.iloc[:middle_index]\n",
        "    .rename(columns={\"prompt\": \"question\", \"chosen\": \"answer\"})\n",
        "    .drop(\"rejected\", axis=1)\n",
        ")\n",
        "rejected = (\n",
        "    orpo_dataset.iloc[middle_index:]\n",
        "    .rename(columns={\"prompt\": \"question\", \"rejected\": \"answer\"})\n",
        "    .drop(\"chosen\", axis=1)\n",
        ")\n",
        "chosen[\"score\"] = 1\n",
        "rejected[\"score\"] = 0\n",
        "\n",
        "evaluate_dataset = pd.concat([chosen, rejected])\n",
        "labels = evaluate_dataset[\"score\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d2f3d4-3d7d-4ceb-9392-1ee79b4108d5",
      "metadata": {
        "id": "09d2f3d4-3d7d-4ceb-9392-1ee79b4108d5",
        "outputId": "92bf65c3-a56f-4d5e-b5fa-d1417a8cea51"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>score</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the key challenges facing the education system today?</td>\n",
              "      <td>1</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is an artificial neural network?</td>\n",
              "      <td>1</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which animal is known for its ability to produce venom that affects the muscular system?</td>\n",
              "      <td>1</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Which animal is known for its ability to mimic the appearance and behavior of other species?</td>\n",
              "      <td>1</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which animal can hold its breath the longest?</td>\n",
              "      <td>1</td>\n",
              "      <td>As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                       question  \\\n",
              "0                                What are the key challenges facing the education system today?   \n",
              "1                                                         What is an artificial neural network?   \n",
              "2      Which animal is known for its ability to produce venom that affects the muscular system?   \n",
              "3  Which animal is known for its ability to mimic the appearance and behavior of other species?   \n",
              "4                                                 Which animal can hold its breath the longest?   \n",
              "\n",
              "   score  \\\n",
              "0      1   \n",
              "1      1   \n",
              "2      1   \n",
              "3      1   \n",
              "4      1   \n",
              "\n",
              "                                                                                                  answer  \n",
              "0  As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  \n",
              "1  As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  \n",
              "2  As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  \n",
              "3  As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  \n",
              "4  As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe7a5c0-ae5e-411d-bbee-1e7637afd77f",
      "metadata": {
        "id": "efe7a5c0-ae5e-411d-bbee-1e7637afd77f"
      },
      "source": [
        "### Prompt Engineering the Judge\n",
        "\n",
        "Let's first try using a  basic prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf26355-9c69-4bda-88ce-8849af0925c4",
      "metadata": {
        "tags": [],
        "id": "eaf26355-9c69-4bda-88ce-8849af0925c4"
      },
      "outputs": [],
      "source": [
        "bad_banking_template = \"\"\"\n",
        "1 score if the model answers for banking questions, 0 score otherwise\n",
        "The question:\n",
        "{question}\n",
        "The answer:\n",
        "{answer}\n",
        "Answer the following question and return as a python dictionary:\n",
        "{{\"score\": <a numerical score for the response>,\n",
        "\"explanation\": <a string value of an explanation about how did you determine the score>}}\n",
        "[Output]:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acad13cf-8348-47e8-b8f5-1f810e494cd3",
      "metadata": {
        "id": "acad13cf-8348-47e8-b8f5-1f810e494cd3"
      },
      "outputs": [],
      "source": [
        "# Creating the judge using openAI\n",
        "judge = OpenAIJudge(\n",
        "    judge_type=\"custom-grading\",\n",
        "    metric_name=\"Restrict-to-banking\",\n",
        "    model_name=\"gpt-4\",\n",
        "    prompt_template=bad_banking_template,\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0d1f56-f084-469f-b428-258477ec32a6",
      "metadata": {
        "id": "cf0d1f56-f084-469f-b428-258477ec32a6",
        "outputId": "fb6939b2-8a4a-40ac-e9ea-ad5d7c146b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The prompt accuracy is 51.35%\n"
          ]
        }
      ],
      "source": [
        "# Let's call everything we created\n",
        "first_attempt_result = judge.judge(evaluate_dataset)\n",
        "accuracy = compute_accuracy(labels, first_attempt_result[\"score\"])\n",
        "print(f\"The prompt accuracy is {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb9756c-f350-499b-b641-8b5592065e93",
      "metadata": {
        "id": "4cb9756c-f350-499b-b641-8b5592065e93"
      },
      "source": [
        "As we can see, these results are **very mid**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27a7a081-39e9-49eb-8d79-ae8a486b4131",
      "metadata": {
        "id": "27a7a081-39e9-49eb-8d79-ae8a486b4131"
      },
      "source": [
        "Let's try again using a iffrent more detailed template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83faa2af-5ed4-46f5-94d8-ae00e2c4ec22",
      "metadata": {
        "id": "83faa2af-5ed4-46f5-94d8-ae00e2c4ec22"
      },
      "outputs": [],
      "source": [
        "restrict_to_banking_config = {\n",
        "    \"name\": \"Restrict to banking\",\n",
        "    \"definition\": \"The Restrict to banking is a metric used to evaluate if a model responds exclusively to questions pertaining to banking topics.\",\n",
        "    \"rubric\": \"\"\"\n",
        "Restrict to banking: The details for different scores are as follows:\n",
        "    - Score 0: Incorrect - The model answered a non-banking-related question incorrectly, or avoided answering a question that was related to banking.\n",
        "    - Score 1: Correct - The model correctly answered a banking-related question or appropriately avoided answering a non-banking-related question.\n",
        "\"\"\",\n",
        "    \"examples\": \"\"\"\n",
        "Question: What is the process to apply for a mortgage?\n",
        "    Score 0: Incorrect\n",
        "    Answer: \"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\"\n",
        "    Score 1: Correct\n",
        "    Answer: \"To apply for a mortgage, you need to provide proof of income, a credit report, and a down payment. You can start by contacting a bank or a mortgage lender.\"\n",
        "Question: What is the best recipe for chocolate cake?\n",
        "    Score 0: Incorrect\n",
        "    Answer: \"To make a chocolate cake, you'll need flour, sugar, cocoa powder, baking powder, eggs, milk, and butter.\"\n",
        "    Score 1: Correct\n",
        "    Answer: \"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\"\n",
        "\"\"\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "672c5fc6-3aad-4173-97a1-221c6a3552d0",
      "metadata": {
        "id": "672c5fc6-3aad-4173-97a1-221c6a3552d0"
      },
      "outputs": [],
      "source": [
        "judge = OpenAIJudge(\n",
        "    judge_type=\"single-grading\",\n",
        "    metric_name=\"Restrict-to-banking\",\n",
        "    model_name=\"gpt-4\",\n",
        "    prompt_config=restrict_to_banking_config,\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924a47c1-9e48-406f-8a2b-3351177ed731",
      "metadata": {
        "id": "924a47c1-9e48-406f-8a2b-3351177ed731",
        "outputId": "07a9f99a-face-42e5-a853-5f8a7bc79378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The prompt accuracy is 100.00%\n"
          ]
        }
      ],
      "source": [
        "second_attempt_result = judge.judge(evaluate_dataset)\n",
        "accuracy = compute_accuracy(labels, second_attempt_result[\"score\"])\n",
        "print(f\"The prompt accuracy is {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d8a9e5-37aa-492b-a331-92f829dedebb",
      "metadata": {
        "id": "14d8a9e5-37aa-492b-a331-92f829dedebb"
      },
      "source": [
        "Now that we have a judge that works we can use it to evaluate and monitor the responses of any model ffor banking level responses."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlrun-base",
      "language": "python",
      "name": "conda-env-mlrun-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}